# Neural ODE Demonstration

This project demonstrates the training of **Neural Ordinary Differential Equations (Neural ODEs)** on a simple spiral dynamical system.

## ðŸ“š What are Neural ODEs?

Neural ODEs replace discrete layers in neural networks with continuous transformations described by ordinary differential equations:

```
dh(t)/dt = f_Î¸(h(t), t)
```

where `f_Î¸` is a neural network with parameters `Î¸`.

### Key Advantages:
- **Memory-efficient training** using the adjoint method (O(1) memory)
- **Adaptive computation** - can evaluate at any time point
- **Natural modeling** of continuous-time processes
- **Fewer parameters** compared to traditional recurrent architectures

## ðŸš€ Getting Started

### Prerequisites

You need Python 3.7+ with the following packages:
- PyTorch
- torchdiffeq
- matplotlib
- numpy

### Installation

Install the required packages:

```bash
pip install -r requirements.txt
```

### Running the Notebook

1. Open `neural_ode_demo.ipynb` in Jupyter Notebook or JupyterLab
2. Run all cells sequentially
3. The notebook will:
   - Generate spiral trajectory data
   - Train a Neural ODE to learn the dynamics
   - Visualize training progress
   - Compare learned vs true dynamics
   - Test extrapolation capabilities

## ðŸ“Š What the Notebook Demonstrates

### 1. **Data Generation**
Creates a spiral trajectory from a known ODE system with initial condition `[2.0, 0.0]`

### 2. **Model Architecture**
Builds a Neural ODE with:
- Input: 2D state (x, y)
- Hidden layer: 50 neurons with Tanh activation
- Output: 2D dynamics

### 3. **Training**
- Uses the adjoint method for memory-efficient backpropagation
- Mini-batch training with random trajectory subsequences
- Adam optimizer with learning rate 1e-3
- 2000 training iterations

### 4. **Evaluation**
- Compares learned dynamics with true dynamics
- Visualizes trajectories, phase portraits, and vector fields
- Tests extrapolation beyond training time range

## ðŸ“ˆ Expected Results

After training, you should see:
- **Training loss** decreasing from ~0.1 to <0.001
- **Phase portrait** matching the true spiral closely
- **Vector field** showing the learned dynamics
- **Good extrapolation** beyond the training time range (0-25s)

## ðŸ”¬ Key Concepts Illustrated

1. **Continuous Depth Networks**: ODEs provide infinite-depth networks
2. **Adjoint Sensitivity Method**: Efficient backpropagation through ODE solvers
3. **Adaptive Computation**: Variable-step ODE solvers adjust based on dynamics
4. **Time-Continuous Models**: Natural representation for physical systems

## ðŸ“– References

- **Original Paper**: [Neural Ordinary Differential Equations (NeurIPS 2018)](https://arxiv.org/abs/1806.07366)
  - Chen, Ricky T. Q., et al.
  
- **Library**: [torchdiffeq](https://github.com/rtqichen/torchdiffeq)
  - PyTorch implementation of differentiable ODE solvers

## ðŸŽ¯ Potential Applications

- Time series forecasting
- Physics-informed machine learning
- Continuous normalizing flows (generative models)
- Robotics and control systems
- Medical signal processing (EEG, ECG)
- Climate modeling
- Chemical kinetics

## ðŸ“„ Documentation

For a comprehensive treatment of Neural ODEs including theoretical foundations, architectural design principles, and systematic optimization techniques, see:
- **`neural_ode_tutorial.tex`**: Complete LaTeX manuscript with 17 pages of detailed analysis
- **`neural_ode_tutorial.pdf`**: Compiled PDF with publication-quality figures
- **`figures/`**: All experimental visualizations

## ðŸ† Key Results

Our systematic investigation demonstrates:
- **86.7% parameter reduction** (4,482 vs 33,666 params) with improved performance
- **Residual architectures** outperform dense networks significantly
- **Time-weighted training** eliminates temporal drift
- **Superior extrapolation** beyond training time range

## ðŸ¤ Contributing

Feel free to experiment with different:
- Dynamical systems (pendulum, Lorenz attractor, etc.)
- Network architectures
- Training strategies
- Visualization techniques

---

Happy learning! ðŸŽ“