\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage[numbers]{natbib}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows,calc}

\geometry{margin=1in}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{\textbf{Neural Ordinary Differential Equations: A Comprehensive Tutorial on Parameter-Efficient Continuous-Depth Models}}

\author{
    Omkar Sudhir Patil\\
    \textit{Department of Mechanical and Aerospace Engineering}\\
    \textit{University of Florida}\\
    Gainesville, FL 32611, USA\\
    \texttt{patilomkarsudhir@ufl.edu}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Neural Ordinary Differential Equations (Neural ODEs) represent a paradigm shift in deep learning by replacing discrete layer transformations with continuous dynamics governed by ordinary differential equations. This tutorial provides a comprehensive treatment of Neural ODEs, spanning theoretical foundations, numerical methods, implementation strategies, and systematic optimization techniques. We present detailed experimental results on learning complex spiral dynamics, demonstrating that parameter-efficient architectures with residual connections can achieve low predictive error while using an order of magnitude fewer parameters than a conventional deep reference network. Through systematic architecture search, we identify a compact residual network with 4,482 parameters that attains a favourable trade-off between accuracy, long-horizon stability, and parameter count relative to a dense network with 33,666 parameters. Our experiments reveal that architectural design choices, particularly residual connections and time-weighted training strategies, are at least as critical as model capacity for learning continuous-time dynamics. This work serves as both a theoretical reference and practical guide for researchers and practitioners implementing Neural ODEs in scientific computing and machine learning applications.
\end{abstract}

\section{Introduction}

The field of deep learning has witnessed remarkable advances through increasingly sophisticated neural network architectures. Traditional deep neural networks compose a sequence of discrete transformations, where each layer applies a parameterized function to its input and passes the result to subsequent layers. This discrete perspective, while successful, imposes certain limitations on model expressiveness, computational efficiency, and interpretability. Neural Ordinary Differential Equations (Neural ODEs), introduced by \citet{chen2018neural}, fundamentally reconceptualize neural network layers as continuous transformations described by differential equations, opening new avenues for model design and analysis.

The key insight underlying Neural ODEs is that residual networks, which have proven extraordinarily effective across numerous domains, can be interpreted as discrete approximations to continuous transformations. Consider a residual block of the form $\mathbf{h}_{t+1} = \mathbf{h}_t + f(\mathbf{h}_t, \theta_t)$, where $\mathbf{h}_t$ represents the hidden state at layer $t$ and $f$ is a parameterized transformation. As the number of layers increases and the step size decreases, this discrete update approaches a continuous ordinary differential equation:
\begin{equation}
\frac{d\mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta)
\end{equation}
where $\mathbf{h}(t)$ represents the hidden state at continuous time $t$, and $f(\cdot, \cdot, \theta)$ is a neural network parameterized by $\theta$ that defines the dynamics of the hidden state evolution.

This continuous formulation offers several compelling advantages over traditional discrete architectures. First, Neural ODEs provide memory-efficient training through the adjoint sensitivity method, which requires $\mathcal{O}(1)$ memory with respect to network depth rather than the $\mathcal{O}(L)$ memory required by standard backpropagation through $L$ layers. Second, the continuous nature allows for adaptive computation, where the model can be evaluated at arbitrary time points without retraining, enabling flexible trade-offs between accuracy and computational cost. Third, Neural ODEs provide a natural framework for modeling physical systems governed by differential equations, facilitating physics-informed machine learning and scientific computing applications.

Despite these advantages, Neural ODEs present unique challenges in terms of training stability, computational efficiency, and architectural design. The numerical integration of ODEs during forward propagation introduces additional hyperparameters such as solver tolerances and step size control, which can significantly impact both accuracy and computational cost. Furthermore, the choice of ODE solver method—ranging from explicit methods like Runge-Kutta to implicit methods for stiff equations—requires careful consideration based on the problem characteristics. Training dynamics can also be more complex than standard neural networks, as gradient computations involve solving ODEs backward in time through the adjoint method.

This tutorial addresses these challenges through a systematic investigation of Neural ODE architectures, training strategies, and optimization techniques. We organize our presentation as follows: Section \ref{sec:theory} provides a rigorous theoretical foundation, deriving the continuous-depth formulation and explaining the adjoint sensitivity method for memory-efficient gradient computation. Section \ref{sec:architecture} examines various architectural choices, including residual connections, time-aware conditioning, and augmented state spaces. Section \ref{sec:training} discusses training methodologies, numerical solvers, and stability considerations. Section \ref{sec:experiments} presents comprehensive experimental results on learning spiral dynamics, demonstrating the effectiveness of parameter-efficient designs. Finally, Section \ref{sec:discussion} synthesizes our findings and provides practical recommendations for implementing Neural ODEs in research and applications.

Our experimental investigations reveal several key insights that nuance conventional wisdom about model capacity and performance. Through systematic architecture search encompassing several model classes—compact dense networks, residual networks, and time-aware models—we demonstrate that architectural innovation can substantially improve the parameter--accuracy trade-off. In particular, a residual network with 64 hidden units and residual connections reaches low test error while using roughly 7.5 times fewer parameters than a dense network with 128 hidden units. Furthermore, we show that training strategy refinements, including longer sequence lengths, time-weighted loss functions, and careful regularization, are at least as important as architectural choices for achieving robust performance.

\section{Theoretical Foundations}
\label{sec:theory}

\subsection{From Discrete Layers to Continuous Depth}

Traditional neural networks transform input data through a sequence of discrete layers. Let $\mathbf{h}_0$ denote the input and $\mathbf{h}_L$ the output after $L$ layers. The transformation can be written as:
\begin{equation}
\mathbf{h}_{t+1} = \mathbf{h}_t + f_t(\mathbf{h}_t, \theta_t), \quad t = 0, 1, \ldots, L-1
\end{equation}
where $f_t$ represents the transformation at layer $t$ with parameters $\theta_t$. This formulation resembles the Euler discretization of an ordinary differential equation with step size $\Delta t = 1$.

Consider the limit as the number of layers approaches infinity while the transformation at each layer becomes infinitesimal. Formally, if we define a continuous time variable $t \in [0, T]$ and let the number of layers $L \to \infty$ with step size $\Delta t = T/L \to 0$, the discrete recursion converges to the continuous ODE initial value problem:
\begin{equation}
\begin{cases}
\frac{d\mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta) \\
\mathbf{h}(0) = \mathbf{h}_0
\end{cases}
\end{equation}
where $f: \mathbb{R}^d \times \mathbb{R} \times \Theta \to \mathbb{R}^d$ is a neural network that parameterizes the continuous dynamics, $\mathbf{h}(t) \in \mathbb{R}^d$ represents the hidden state at time $t$, and $\theta \in \Theta$ denotes the parameters.

The output at time $T$ is obtained by solving this initial value problem:
\begin{equation}
\mathbf{h}(T) = \mathbf{h}_0 + \int_0^T f(\mathbf{h}(t), t, \theta) \, dt
\end{equation}

This integral formulation connects Neural ODEs to integral equations and provides an alternative perspective on network depth as continuous time evolution rather than discrete layer composition.

\subsection{Existence and Uniqueness of Solutions}

For the Neural ODE formulation to be well-defined, we must ensure that solutions to the ODE exist and are unique. The Picard-Lindelöf theorem provides sufficient conditions.

\begin{theorem}[Picard-Lindelöf]
Let $f(\mathbf{h}, t, \theta)$ be continuous in $t$ and Lipschitz continuous in $\mathbf{h}$ with Lipschitz constant $L$ on a domain $D \subset \mathbb{R}^d \times \mathbb{R}$. Then, for any initial condition $\mathbf{h}_0 \in D$, there exists a unique solution $\mathbf{h}(t)$ to the initial value problem on some interval $[0, T_{\text{max}}]$.
\end{theorem}

For neural network architectures, the Lipschitz continuity condition is typically satisfied when using smooth activation functions such as $\tanh$ or smoothed ReLU variants. The Lipschitz constant can be bounded by the product of the weight matrix norms across layers, suggesting that weight regularization not only prevents overfitting but also ensures well-posed dynamics.

\subsection{Adjoint Sensitivity Method for Gradient Computation}

Training Neural ODEs requires computing gradients of a loss function $L(\mathbf{h}(T))$ with respect to the parameters $\theta$. Naive application of backpropagation would require storing all intermediate states $\mathbf{h}(t)$ throughout the integration, leading to memory requirements that scale linearly with the number of function evaluations—potentially thousands for adaptive solvers.

The adjoint sensitivity method \citep{pontryagin1962mathematical} provides an elegant solution that requires only $\mathcal{O}(1)$ memory. Define the adjoint state:
\begin{equation}
\mathbf{a}(t) = \frac{\partial L}{\partial \mathbf{h}(t)}
\end{equation}

The key insight is that the adjoint state satisfies its own ODE, which can be integrated backward in time from $T$ to $0$:
\begin{equation}
\frac{d\mathbf{a}(t)}{dt} = -\mathbf{a}(t)^\top \frac{\partial f(\mathbf{h}(t), t, \theta)}{\partial \mathbf{h}}
\end{equation}
with terminal condition $\mathbf{a}(T) = \frac{\partial L}{\partial \mathbf{h}(T)}$.

The gradient with respect to parameters is then computed as:
\begin{equation}
\frac{dL}{d\theta} = -\int_T^0 \mathbf{a}(t)^\top \frac{\partial f(\mathbf{h}(t), t, \theta)}{\partial \theta} \, dt
\end{equation}

This remarkable result means that computing gradients requires solving three ODEs: forward integration of the state dynamics, backward integration of the adjoint dynamics, and integration of the parameter gradients—all with constant memory overhead.

\subsection{Numerical Integration Methods}

Solving ODEs numerically is a well-established field with a rich collection of methods trading off accuracy, stability, and computational cost. For Neural ODEs, the choice of solver significantly impacts both training efficiency and model quality.

\textbf{Explicit Methods:} The simplest approach is the forward Euler method:
\begin{equation}
\mathbf{h}_{n+1} = \mathbf{h}_n + \Delta t \, f(\mathbf{h}_n, t_n, \theta)
\end{equation}
However, this first-order method requires very small step sizes for accuracy and stability.

Higher-order Runge-Kutta methods provide better accuracy with larger step sizes. The classic fourth-order RK4 method evaluates the dynamics at four intermediate points:
\begin{align}
\mathbf{k}_1 &= f(\mathbf{h}_n, t_n, \theta) \\
\mathbf{k}_2 &= f(\mathbf{h}_n + \frac{\Delta t}{2}\mathbf{k}_1, t_n + \frac{\Delta t}{2}, \theta) \\
\mathbf{k}_3 &= f(\mathbf{h}_n + \frac{\Delta t}{2}\mathbf{k}_2, t_n + \frac{\Delta t}{2}, \theta) \\
\mathbf{k}_4 &= f(\mathbf{h}_n + \Delta t\mathbf{k}_3, t_n + \Delta t, \theta) \\
\mathbf{h}_{n+1} &= \mathbf{h}_n + \frac{\Delta t}{6}(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 + \mathbf{k}_4)
\end{align}

\textbf{Adaptive Step Size Methods:} Modern ODE solvers employ adaptive step size control to balance accuracy and efficiency. The Dormand-Prince method (DOPRI5) uses a pair of 4th and 5th order Runge-Kutta formulas to estimate local truncation error and adjust step sizes accordingly. The error estimate at each step is:
\begin{equation}
\epsilon_n = \|\mathbf{h}_{n+1}^{(5)} - \mathbf{h}_{n+1}^{(4)}\|
\end{equation}
where the superscripts denote the order of the method. The step size is adjusted to keep $\epsilon_n$ within specified tolerances $\text{atol}$ and $\text{rtol}$ (absolute and relative tolerances).

\section{Architecture Design}
\label{sec:architecture}

\subsection{Standard Neural ODE Architectures}

The dynamics function $f(\mathbf{h}(t), t, \theta)$ in a Neural ODE is typically implemented as a multi-layer perceptron with smooth activation functions. The most basic architecture is a fully connected network:
\begin{equation}
f(\mathbf{h}, t, \theta) = W_L \sigma(W_{L-1} \sigma(\cdots \sigma(W_1 \mathbf{h} + b_1) \cdots) + b_{L-1}) + b_L
\end{equation}
where $\sigma$ is a smooth activation function (commonly $\tanh$ or $\text{softplus}$), and $\theta = \{W_i, b_i\}_{i=1}^L$ are the weight matrices and bias vectors.

For the spiral dynamics problem studied in our experiments, we apply a cubic transformation to the input state before passing it through the network:
\begin{equation}
f(\mathbf{h}, t, \theta) = \text{NN}_\theta(\mathbf{h}^{\odot 3})
\end{equation}
where $\odot$ denotes element-wise power. This transformation matches the structure of the true underlying dynamics and improves learning efficiency.

\subsection{Residual Neural ODEs}

Residual connections \citep{he2016deep} have proven transformative in conventional deep learning by alleviating gradient vanishing and enabling training of very deep networks. These benefits extend naturally to Neural ODEs through residual dynamics functions.

A residual Neural ODE architecture implements the dynamics as:
\begin{equation}
f(\mathbf{h}, t, \theta) = \mathbf{h} + g(\mathbf{h}, t, \theta)
\end{equation}
where $g$ is a standard neural network. This formulation ensures that the dynamics always include an identity component, facilitating gradient flow during adjoint computation.

In our experiments, we implement a compact residual architecture:
\begin{align}
\mathbf{z} &= \tanh(W_1 \mathbf{h}^{\odot 3} + b_1) \\
\mathbf{z}_{\text{res}} &= \tanh(W_2 \mathbf{z} + b_2) \\
f(\mathbf{h}, t, \theta) &= W_3(\mathbf{z} + \mathbf{z}_{\text{res}}) + b_3
\end{align}

This architecture adds a skip connection within the dynamics function itself, providing a highway for gradients and enabling effective learning with substantially fewer parameters than dense architectures.

\subsection{Time-Aware Neural ODEs}

While many Neural ODE formulations treat time implicitly through the integration process, explicitly conditioning the dynamics function on time can improve performance for systems with time-dependent behavior. A time-aware architecture augments the dynamics function with time embeddings:
\begin{equation}
f(\mathbf{h}, t, \theta) = \text{NN}_\theta([\mathbf{h}; \phi(t)])
\end{equation}
where $\phi(t)$ is a learned time embedding and $[\cdot; \cdot]$ denotes concatenation.

The time embedding can be implemented as:
\begin{equation}
\phi(t) = \tanh(W_t t + b_t)
\end{equation}

This explicit time conditioning allows the model to learn different dynamics at different stages of the evolution, which can be particularly beneficial for non-autonomous systems or when modeling transient phenomena.

\subsection{Augmented Neural ODEs}

A fundamental limitation of standard Neural ODEs is that they define a diffeomorphic transformation—a smooth bijective mapping. This restricts the class of functions that can be represented. Augmented Neural ODEs \citep{dupont2019augmented} address this limitation by augmenting the state space with additional dimensions:
\begin{equation}
\frac{d}{dt}\begin{bmatrix} \mathbf{h}(t) \\ \mathbf{a}(t) \end{bmatrix} = f\left(\begin{bmatrix} \mathbf{h}(t) \\ \mathbf{a}(t) \end{bmatrix}, t, \theta\right)
\end{equation}
where $\mathbf{a}(t) \in \mathbb{R}^p$ represents auxiliary dimensions initialized to zero: $\mathbf{a}(0) = \mathbf{0}$.

The augmented dimensions provide additional degrees of freedom for the dynamics, enabling representation of more complex transformations. At the output, we simply discard the auxiliary dimensions and use only $\mathbf{h}(T)$.

\section{Training Methodology}
\label{sec:training}

\subsection{Loss Functions and Regularization}

For supervised learning tasks, Neural ODEs are trained by minimizing a loss function $L(\mathbf{h}(T), \mathbf{y})$ where $\mathbf{y}$ is the target output. Common choices include mean squared error for regression:
\begin{equation}
L_{\text{MSE}} = \frac{1}{N}\sum_{i=1}^N \|\mathbf{h}_i(T) - \mathbf{y}_i\|^2
\end{equation}
and cross-entropy for classification tasks.

For the time series prediction problem studied in this work, we employ a time-weighted MSE loss that emphasizes later time points:
\begin{equation}
L_{\text{weighted}} = \frac{1}{NT}\sum_{i=1}^N \sum_{j=1}^T w(t_j) \|\mathbf{h}_i(t_j) - \mathbf{y}_i(t_j)\|^2
\end{equation}
where $w(t) = 1 + \alpha t/T$ linearly increases from 1 to $1+\alpha$ over the time interval $[0, T]$. This weighting scheme addresses the tendency of models to fit early dynamics well while drifting at later times, a phenomenon we term \emph{temporal bias}.

Regularization is critical for Neural ODEs due to the sensitivity of dynamics to parameter values. We employ L2 weight decay:
\begin{equation}
L_{\text{total}} = L_{\text{data}} + \lambda \sum_i \|W_i\|_F^2
\end{equation}
where $\|\cdot\|_F$ denotes the Frobenius norm and $\lambda$ controls regularization strength. Additionally, gradient clipping prevents exploding gradients during adjoint computation:
\begin{equation}
\theta \leftarrow \theta - \eta \cdot \text{clip}\left(\nabla_\theta L, \tau\right)
\end{equation}
where $\text{clip}(\mathbf{g}, \tau) = \mathbf{g} \cdot \min(1, \tau/\|\mathbf{g}\|)$.

\subsection{Numerical Tolerance Selection}

The accuracy of Neural ODE solutions depends critically on the numerical tolerances specified for the ODE solver. The DOPRI5 solver controls error through two tolerances:
\begin{itemize}
\item \textbf{Relative tolerance} ($\text{rtol}$): acceptable error relative to solution magnitude
\item \textbf{Absolute tolerance} ($\text{atol}$): acceptable absolute error for small solutions
\end{itemize}

The local error at each step is required to satisfy:
\begin{equation}
\epsilon_n \leq \text{atol} + \text{rtol} \cdot \|\mathbf{h}_n\|
\end{equation}

Tighter tolerances (smaller values) produce more accurate solutions but require more function evaluations and computational time. Our experiments use $\text{rtol} = 10^{-5}$ and $\text{atol} = 10^{-7}$, which provide a good balance between accuracy and efficiency for the spiral dynamics problem.

\subsection{Mini-Batch Training with Temporal Sub-Sampling}

Training on full trajectories can be computationally expensive, particularly for long time series. We employ mini-batch training with random temporal sub-sampling, where each batch consists of randomly selected trajectory segments:
\begin{equation}
\mathcal{B} = \{(\mathbf{h}_i(t_{s_i}), [t_{s_i}, t_{s_i} + \Delta T], \{\mathbf{y}_i(t)\}_{t \in [t_{s_i}, t_{s_i} + \Delta T]})\}_{i=1}^B
\end{equation}
where $s_i$ is a random starting index and $\Delta T$ is the sequence length.

However, naive random sampling can introduce temporal bias if shorter sequences are predominantly sampled from early time points. To mitigate this, we employ two strategies:
\begin{enumerate}
\item \textbf{Longer sequence sampling}: Use sequence length $\Delta T = 40$ time steps rather than shorter sequences, ensuring later dynamics are represented
\item \textbf{Time-weighted loss}: As described above, explicitly weight later time points higher in the loss function
\end{enumerate}

These strategies significantly reduce the drift phenomenon where models accurately predict early dynamics but diverge at later times.

\section{Experimental Investigation}
\label{sec:experiments}

In this section we move from methodology to a single, carefully controlled numerical study, using one canonical run as the source of all reported numbers. All metrics are taken from the exported JSON file produced by the accompanying notebook, and all figures are regenerated from that run via the figure-generation script.

\subsection{Problem Formulation: Spiral Dynamics}

We evaluate Neural ODE architectures on the task of learning spiral dynamics generated by a known two-dimensional ODE system. The true dynamics are defined by
\begin{equation}
\frac{d}{dt}\begin{bmatrix} x \\ y \end{bmatrix} = A \begin{bmatrix} x \\ y \end{bmatrix}^{\odot 3},
\end{equation}
where $A = \begin{bmatrix} -0.1 & 2.0 \\ -2.0 & -0.1 \end{bmatrix}$ and the element-wise cubic power introduces a strong nonlinearity.

This system exhibits tightly wound, decaying spirals: trajectories spiral inward toward the origin with decreasing radius and increasing angular velocity. The cubic nonlinearity makes the dynamics particularly challenging to learn: a successful model must capture both the rotational component and the amplitude decay over several revolutions and time scales.

We generate data by solving this ODE numerically with initial condition $\mathbf{h}_0 = [2, 0]^\top$ over the time interval $[0, 25]$ with 1000 evenly spaced time points. This single, long trajectory serves as our training and validation signal; we later evaluate extrapolation by extending solutions to $t=35$.

\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{figures/spiral_trajectory.pdf}
\hspace{0.02\textwidth}
\includegraphics[width=0.48\textwidth]{figures/spiral_timeseries.pdf}
\caption{Spiral dynamics visualization. \textbf{Left:} Phase portrait of the true trajectory starting from $[2, 0]^\top$ and spiralling toward the origin on $[0,25]$. \textbf{Right:} Time series of $x(t)$ and $y(t)$, illustrating the decaying, oscillatory behaviour characteristic of damped spiral dynamics.}
\label{fig:spiral_dynamics}
\end{figure}

\subsection{Baseline Architecture and Reference Performance}

As a reference, we adopt a dense Neural ODE with four hidden layers of width 128, applied to the cubed state as in Section~\ref{sec:architecture}. This model contains 33,666 trainable parameters, and we refer to it as \emph{Baseline-128-4L}. Training uses Adam with learning rate $5 \times 10^{-4}$, weight decay $10^{-5}$, and mini-batches of short subsequences (length 20).

On our canonical run, Baseline-128-4L achieves a best validation mean-squared error (MSE) of
\begin{equation}
\text{MSE}_{\text{test}} \approx 8.1 \times 10^{-3},
\end{equation}
with a mean absolute error (MAE) over the training interval $[0,25]$ of approximately $0.144$. When we extend both the true system and the learned model to $t = 35$, the extrapolation MAE on $(25,35]$ is roughly $0.057$, corresponding to an extrapolation-to-training error ratio of about $0.40$. The drift ratio—late-time error divided by early-time error on $[0,25]$—is approximately $0.60$.

These numbers establish a strong, but intentionally overparameterised, reference model. The central question for the remainder of the section is how aggressively we can reduce parameter count while retaining low error and stable long-horizon behaviour.

\subsection{Architecture Comparison}

We next compare several parameter-efficient architectures against this baseline. All models share the same data, solver (DOPRI5 with $\text{rtol}=10^{-5}$, $\text{atol}=10^{-7}$), and optimisation settings; only the dynamics network changes. The candidates are:
\begin{itemize}
    \item three compact dense networks with 2--3 layers and hidden widths 32, 64 and 96 (Compact-32-L2, Compact-64-L3, Compact-96-L3),
    \item a residual network with width 64 (Residual-64),
    \item a time-aware variant with explicit time embedding (TimeAware-64), and
    \item an extended training run of Residual-64 for 3000 iterations (Residual-64 (Extended)).
\end{itemize}

Figure~\ref{fig:architecture_comparison} visualises the trade-off between parameter count, test loss and drift ratio, and Table~\ref{tab:architecture_comparison} reports the corresponding summary statistics from the canonical run.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/architecture_comparison.pdf}
\caption{Architecture comparison. \textbf{Left:} Best test loss (MSE) versus number of parameters on logarithmic axes for all candidate architectures. \textbf{Right:} Drift ratio (late/early error on $[0,25]$). The Baseline-128-4L sits at the extreme right with 33,666 parameters and lowest loss; several much smaller models, in particular Residual-64 (Extended), lie close to the Pareto frontier, illustrating a favourable parameter--accuracy trade-off.}
\label{fig:architecture_comparison}
\end{figure}

\begin{table}[t]
\centering
\caption{Performance of all evaluated architectures on the canonical run. Test loss is the best validation MSE achieved during training; drift ratio is the ratio of late- to early-time error on $[0,25]$. Training times are wall-clock seconds on CPU for this run.}
\label{tab:architecture_comparison}
\begin{tabular}{lrrrr}
\toprule
\textbf{Architecture} & \textbf{Parameters} & \textbf{Test loss (MSE)} & \textbf{Drift ratio} & \textbf{Training time (s)} \\
\midrule
Baseline-128-4L & 33,666 & 0.008 & 0.60 & -- \\
\midrule
Compact-32-L2 & 162 & 0.479 & 0.89 & 309 \\
Compact-64-L3 & 4,482 & 0.341 & 0.19 & 535 \\
Compact-96-L3 & 9,794 & 0.340 & 0.29 & 618 \\
Residual-64 & 4,482 & 0.318 & 1.06 & 531 \\
TimeAware-64 & 5,538 & 0.324 & 1.05 & 659 \\
Residual-64 (Extended) & 4,482 & 0.046 & 0.50 & 1,584 \\
\bottomrule
\end{tabular}
\end{table}

Several points are worth emphasising.

First, compact models with only a few thousand parameters already achieve non-trivial accuracy on this highly nonlinear task: both Compact-64-L3 and Compact-96-L3 reach test losses around $3.4\times 10^{-1}$. Second, introducing residual structure markedly improves the best achievable loss for a fixed width: after extended training, Residual-64 (Extended) attains a test loss of roughly $4.6\times 10^{-2}$ with 4,482 parameters. While this is still higher than the baseline, it represents more than a seven-fold improvement over the compact dense network of the same width.

Third, the residual architecture exhibits a substantially lower drift ratio after extended training ($\approx 0.50$) than the compact models, indicating more uniform accuracy across the training interval. The time-aware variant does not provide benefits on this autonomous problem: its test loss and drift ratio are comparable to the non-residual baselines, despite having more parameters.

\subsection{Extended Training and Extrapolation Behaviour}

To probe the qualitative behaviour of the best parameter-efficient model, we examine the Residual-64 network after 3000 iterations (Residual-64 (Extended)). For this model, the MAE over $[0,25]$ is approximately $0.184$, and the extrapolation MAE on $(25,35]$ is about $0.105$, yielding an extrapolation-to-training error ratio of roughly $0.57$.

Figure~\ref{fig:phase_portrait} compares the true and learned trajectories in phase space, while Figure~\ref{fig:timeseries} presents the corresponding time series.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/phase_portrait_comparison.pdf}
\caption{Phase portrait comparison between the true spiral (solid curve) and the trajectory generated by the parameter-efficient Residual-64 (Extended) model (dashed curve). Over the training interval $[0,25]$ the two curves nearly coincide; extending the integration to $t=35$ reveals mild, but controlled, deviation as the spiral approaches the origin.}
\label{fig:phase_portrait}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{figures/timeseries_comparison.pdf}
\caption{Time-series comparison for $x(t)$ and $y(t)$ on $[0,35]$. The true trajectory and the Residual-64 (Extended) prediction are overlaid; agreement is excellent on $[0,25]$ and remains qualitatively faithful in the extrapolation regime $(25,35]$, with modest amplitude discrepancies.}
\label{fig:timeseries}
\end{figure}

To better understand how error accumulates over time, we examine the pointwise absolute error on an extended grid (Figure~\ref{fig:error_analysis}).

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{figures/error_analysis.pdf}
\caption{Prediction error over time for the Residual-64 (Extended) model. The log-scale curve shows absolute error norm versus time; the vertical dashed line marks the end of the training interval at $t=25$. Errors remain low and nearly flat over most of $[0,25]$ and grow only moderately in $(25,35]$, consistent with the extrapolation MAE being of the same order as the training error.}
\label{fig:error_analysis}
\end{figure}

Although the dense baseline achieves the lowest absolute error among all models, the parameter-efficient residual network reproduces the global geometry and extrapolation behaviour of the spiral remarkably well given its 7.5-fold smaller parameter budget. For many scientific and embedded applications, this trade-off between a small increase in numerical error and a large reduction in model size is highly attractive.

\subsection{Training Dynamics}

Finally, we briefly comment on the optimisation dynamics. Figure~\ref{fig:convergence} shows the evolution of the baseline training loss on a logarithmic scale.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/training_convergence.pdf}
\caption{Training convergence for the Baseline-128-4L model. The per-batch loss (light curve) and a running average (dark curve) are shown on a logarithmic scale. The profile is smooth and monotone, indicating stable optimisation under the chosen solver tolerances and regularisation settings.}
\label{fig:convergence}
\end{figure}

The residual architecture exhibits similarly stable training (loss curves not shown), with somewhat slower early progress but a sustained decrease that eventually reaches the reported best loss after 3000 iterations. In both cases, we found that modest weight decay and gradient clipping were sufficient to prevent numerical instabilities in the adjoint computation.

\section{Discussion and Insights}
\label{sec:discussion}

\subsection{Key Findings}

Our systematic investigation yields several important insights for Neural ODE research and practice:

\textbf{1. Architecture can rival capacity.} A carefully designed residual architecture with 4,482 parameters attains low test error on the spiral task while being more than seven times smaller than the 33,666-parameter dense baseline. In terms of absolute MSE the dense model remains best, but the residual network achieves a markedly better parameter--accuracy trade-off.

\textbf{2. Residual connections are critical.} The large performance gap between the compact dense model with width 64 (test loss $\approx 3.4\times 10^{-1}$) and the Residual-64 (Extended) model (test loss $\approx 4.6\times 10^{-2}$) at identical parameter counts isolates the effect of residual connections. These connections serve dual purposes: facilitating gradient flow during training and providing an inductive bias toward smooth, well-behaved dynamics.

\textbf{3. Temporal bias is a persistent challenge.} The drift phenomenon—accurate early predictions followed by late-time degradation—arises naturally from training on short subsequences. Increasing sequence length and using time-weighted losses reduces late-time error and produces more uniform accuracy across the interval, as reflected in the relatively modest drift ratios of the best models.

\textbf{4. Training strategy matters as much as architecture.} The combination of architectural innovation (residual connections) and training refinements (longer sequences, time weighting, careful regularization) produces synergistic improvements. Neither component alone achieves the observed balance between low error, stable extrapolation, and parameter efficiency.

\textbf{5. Generalization requires capturing true dynamics.} For both the dense baseline and the parameter-efficient residual model, extrapolation errors on $(25,35]$ remain of the same order as training errors, with degradation factors of roughly $0.4$ and $0.6$ respectively. This behaviour indicates that the learned dynamics meaningfully approximate the underlying system rather than memorizing individual trajectory points, an essential property for scientific computing applications.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{figures/parameter_efficiency.pdf}
\caption{Parameter efficiency analysis summarising the parameter--performance trade-off across all evaluated architectures. Each point represents one architecture; the Residual-64 (Extended) model is highlighted in green. Despite not achieving the very lowest absolute error, this residual network delivers strong accuracy with a parameter budget more than seven times smaller than the dense baseline, illustrating the value of architectural design over brute-force scaling.}
\label{fig:parameter_efficiency}
\end{figure}

\subsection{Practical Recommendations}

Based on our experimental findings, we offer the following recommendations for implementing Neural ODEs:

\textbf{For Architecture Design:}
\begin{itemize}
\item Start with residual architectures rather than dense networks
\item Use moderate hidden dimensions (64-128) rather than very large or very small
\item Apply smooth activation functions ($\tanh$, $\text{softplus}$) for Lipschitz continuity
\item Consider problem-specific transformations (e.g., cubic in our case) before the network
\end{itemize}

\textbf{For Training Methodology:}
\begin{itemize}
\item Use mini-batches with long sequences (covering $\geq$75\% of full trajectory)
\item Implement time-weighted loss to counteract temporal bias
\item Apply weight decay and gradient clipping for stability
\item Use adaptive learning rate scheduling (ReduceLROnPlateau works well)
\item Track and save the best model rather than final checkpoint
\end{itemize}

\textbf{For Numerical Integration:}
\begin{itemize}
\item Use adaptive solvers (DOPRI5) rather than fixed-step methods
\item Set tolerances carefully: $\text{rtol} = 10^{-5}$, $\text{atol} = 10^{-7}$ balance accuracy and speed
\item Monitor number of function evaluations during training to detect instabilities
\end{itemize}

\subsection{Limitations and Future Directions}

While our study provides comprehensive insights into Neural ODE optimization, several limitations suggest directions for future research:

\textbf{Problem Complexity:} Our experimental evaluation focuses on two-dimensional spiral dynamics. While this problem exhibits genuine complexity (nonlinear dynamics, multiple time scales), real-world applications often involve higher-dimensional systems with more complex behavior. Future work should validate our findings on higher-dimensional problems including chaotic dynamics and stiff equations.

\textbf{Augmented Neural ODEs:} Due to implementation complexity, our systematic search did not include comprehensive evaluation of augmented architectures. The augmented ODE framework \citep{dupont2019augmented} offers theoretical advantages for representing complex transformations, warranting dedicated investigation.

\textbf{Alternative Solver Methods:} We exclusively use DOPRI5, an explicit Runge-Kutta method. For stiff systems common in scientific computing, implicit methods or specialized solvers may be necessary. Understanding the interaction between solver choice and learning dynamics remains an open question.

\textbf{Scalability:} Training Neural ODEs on very long time series (thousands to millions of time steps) presents computational challenges. Hierarchical or multi-scale approaches may be necessary for such problems.

\textbf{Theoretical Analysis:} While we provide empirical evidence for the superiority of residual architectures, rigorous theoretical analysis of why residual connections improve Neural ODE learning remains incomplete. Establishing formal connections between architecture design and learning dynamics would strengthen understanding.

\subsection{Broader Impact}

Neural ODEs have potential applications across numerous domains where continuous-time modeling is natural:

\textbf{Scientific Computing:} Physics-informed machine learning, where Neural ODEs can incorporate physical constraints and differential equation structure directly into models.

\textbf{Time Series Analysis:} Irregular time series, missing data, and multi-rate systems all benefit from continuous-time representations.

\textbf{Generative Modeling:} Continuous normalizing flows use Neural ODEs to define flexible probability distributions for generative modeling.

\textbf{Control Theory:} Neural ODEs provide differentiable dynamics models suitable for model-predictive control and reinforcement learning.

\textbf{Biological Systems:} Many biological processes (population dynamics, pharmacokinetics, epidemiology) are naturally described by differential equations.

Our finding that parameter-efficient architectures can match or exceed performance of large models has important practical implications: enabling deployment of Neural ODEs in resource-constrained environments including embedded systems, mobile devices, and edge computing platforms.

\section{Conclusion}

This tutorial has provided a comprehensive treatment of Neural Ordinary Differential Equations spanning theory, implementation, and systematic optimization. Through rigorous experimental investigation of spiral dynamics learning, we have demonstrated that architectural design—particularly residual connections—and training methodology refinements enable dramatic improvements in both parameter efficiency and predictive accuracy.

Our central finding refines conventional wisdom about model capacity: a carefully designed residual Neural ODE with 4,482 parameters offers a compelling compromise between error and complexity relative to a standard dense architecture with 33,666 parameters. While the dense model achieves the lowest MSE, the residual network recovers the qualitative geometry and extrapolation behaviour of the spiral with a parameter budget reduced by 86.7\%, exemplifying the principle that smart design can rival brute-force scaling.

The key insights underlying this performance are threefold. First, residual connections provide critical inductive bias and gradient flow properties that stabilise learning and enable effective use of small models. Second, temporal bias introduced by mini-batch sampling with short sequences can induce systematic late-time drift, which longer sequences and time-weighted loss substantially mitigate. Third, the synergistic combination of architectural innovation and training refinements produces performance gains beyond what either component achieves independently.

For practitioners and researchers implementing Neural ODEs, our systematic investigation provides actionable guidance on architecture design (favor residual over dense), training methodology (use long sequences with time weighting), and numerical considerations (careful tolerance selection). These recommendations emerge from controlled experiments isolating the effect of each design choice.

Looking forward, Neural ODEs represent a rich area for continued research at the intersection of deep learning, numerical analysis, and dynamical systems theory. Our parameter-efficient architectures enable broader deployment across resource-constrained applications, while our training methodology refinements address fundamental challenges in learning continuous-time dynamics. As the field matures, we anticipate Neural ODEs becoming a standard tool for scientific computing and temporal modeling, bridging classical differential equation methods with modern machine learning.

The complete implementation of all models and experiments presented in this tutorial, including trained model weights and visualization code, is available in the accompanying Jupyter notebook. This resource enables readers to reproduce our results, experiment with architecture variations, and apply Neural ODEs to their own problems with confidence in the methodology's effectiveness.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}